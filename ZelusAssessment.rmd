---
title: "Zelus Data Analysis Assessment"
author: "Michael Schatz"
date: "10/9/2021"
output: html_document
---
  
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE)

## Read in initial packages 
library(jsonlite)
library(dplyr)
library(googledrive)
library(purrr)
library(scales)
library(ggplot2)
library(future)
library(tidyr)
library(knitr)

##Read In Data 

## Default parsing of match results is a little messy so setting simplify vector to FALSE and will parse through myself to get relevant information in a hopefully cleaner format 

match_results = fromJSON("https://drive.google.com/u/0/uc?id=19hVoi9f7n7etcmSXx7WHeiDp9pOLpQvN&export=download", 
                simplifyVector = TRUE)

##Innings results too large to read in directly so will need to download with googledrive package first then read in. When running this will require some interactivity to work the first time as the user will need to authenticate. If productionizing  I would put this into a separate script or find a more robust way to make the data accessible.  

##I have commented out the download line so the file does not get redownloaded each time I knit the document. 

# drive_download(as_id("1wQO9zr1VH8bY2W4Ca6cMxPdAoPOHo6X6"), "innings_results.json", overwrite = TRUE)

##Read in innings results. Data reads in a little cleaner by default so can leave in its simplified state 
inning_results = fromJSON("innings_results.json", simplifyVector = TRUE)

```



## Question 1. 

I would rate my knowledge of cricket as a 2. I knew of the sport and the goal to knock down wickets and score runs by running back and forth. I also knew of the Test and T20 distinctions but did not know the difference between the two, but not much else. I could not name any teams or players. Did not know of any positions outside of the batsment or bowler. I did once accidentally knock a friend's tooth out with a cricket bat when I was 10 (but that's a story for another time).

![](https://media.giphy.com/media/BsQAVgY6ksvIY/giphy.gif)



## Question 1. 


```{r q1_checks, echo=FALSE, include=FALSE}

## Checking that there are no instances where there isn't just two records per matchid 
match_results %>% count(matchid) %>% filter(n!=2)

## Check what the possible values for "result" column are and see that NA indicates that there as a winning/losing team. so will now just need to identify DLS method events. 
match_results %>% count(result)

## See that there is an outcome.result column as well. Unsure the difference between outcome.result and result columns, but will assume that ties in that column should be removed as well. 
match_results %>% count(outcome.result)

## Check outcome.method and see that the D/L value seems to indicate the DLS method matches. Any NA value appears to be a result that we want to include 
match_results %>% count(outcome.method)

## Here we see that there are also 2 matches (4 rows) where there is still a NA outcome.winner column even with the filter of ties and no.result. We'll chalk that up to messy dat and remove those from our data set as well. 
 match_results %>% 
  filter(is.na(result) & is.na(outcome.method)) %>% 
   count(outcome.winner)

```

First we filter out any no results, ties and DLS matches by finding NA values for results and outcome.method
```{r q1_base, echo=TRUE}
match_results_use <- match_results %>% 
  filter(is.na(result) & is.na(outcome.method) & is.na(outcome.result) & !is.na(outcome.winner)  &
           lubridate::year(dates) == 2019) 

valid_row_pct = sprintf("%1.1f%%", 100*nrow(match_results_use)/
                          nrow(match_results %>% filter(lubridate::year(dates) == 2019))) 

```

We can check and see that we are left with `r valid_row_pct` of rows in the filtered set for 2019.  

```{r q1, echo=TRUE, warning=FALSE}
match_records = match_results_use %>% 
  mutate(Won = outcome.winner == teams) %>%  # define a win by if the winner column is equal to the team column for a given match
  group_by(teams, gender) %>% 

  summarise(Total_Wins = sum(Won), Win_PCT = scales::percent(mean(Won), .1), Total_Matches = n(), Win_PCT_numeric = mean(Won)) 

```

Below we see the results for male and female teams with the highest win percentage. The **female Australian team** and the **male Netherlands** team had the highest win percentages both at 100% in 2019. 

```{r q1_pct, echo=TRUE, include = TRUE}
match_records %>% 
  group_by(gender) %>% 
  filter(rank(desc(Win_PCT_numeric)) == 1)  %>% 
  knitr::kable()
        
```

And here are the results for male and female teams with the highest win totals. In total wins both the **Australian male and female** teams led the pack with 10 and 16 wins respectively. The female leaders remained the same, but the male leader in win% vs total wins was different. 

```{r q1_total, echo=TRUE, include = TRUE}
match_records %>% 
  group_by(gender) %>% 
  filter(rank(desc(Total_Wins)) == 1) %>% knitr::kable()
        
```

One of the most important distinctions between these two numbers is the number of matches that each team played is not constant. While the male Netherlands team had a 100% win percentage, they also only played in 1 match in all of 2019. Meanwhile the Australian female team played in 10 matches an won all 10. Likewise the Australian male team won 16 matches, but also played in 23 matches (tied for the most of any team with India). While neither in the lead in total wins or win percentage, we can see in the table below that the male England team played in 19 matches and won 14 of them (73.7 Win%) which is higher than Australia's win% of 69.6%. 

```{r q1_total_matches, echo=TRUE, include = TRUE}
match_records %>% 
  group_by(gender) %>% 
  filter(rank(desc(Total_Matches)) <= 8) %>% arrange(gender, desc(Total_Matches)) %>%   
  knitr::kable()
        
```

Aside from denominators being important, quality of opponent likely is not constant between teams as well. Looking at the male Netherland's one match in the set, we see that they played Zimbabwe. I do not know much about the quality of each country's team, but would think that Zimbabwe is a weaker team than say India or Pakistan (which the male Australian team played 9 and 6 times respectively)  

```{r q1_competition, echo=FALSE, include = FALSE}

match_results_use %>% filter(teams!="Netherlands" & gender == "male") %>% 
  inner_join( match_results_use %>% filter(teams=="Netherlands" & gender == "male") %>% select(matchid), by = "matchid") %>%
  count(teams) 

match_results_use %>% filter(teams!="Australia" & gender == "male") %>% 
  inner_join(match_results_use %>% filter(teams=="Australia" & gender == "male") %>% select(matchid), by = "matchid") %>% 
  count(teams) %>% 
  arrange(desc(n))
        
```

## Question 2. 


### 2a. 

To show that the total rating of $R_{i} + R_{j}$ are equal after each update, we can start by looking at the update function itself of 

$R_{i} =  R_{i} + K(W_{ij} - p_{ij})$ and $R_{j} =  R_{j} + K(W_{ji} - p_{ji})$. 

Since the rating update formulas are linear, should the assumption of total ratings being equal hold true then 
$W_{ji} - p_{ji} = -1 * (W_{ij} - p_{ij})$
or 
$W_{ji} - p_{ji} = - W_{ij} + p_{ij})$

Since $p_{ji}$ is an expected win probability, we can also state that  $p_{ji} = 1 - p_{ij}$.   

There are 3 possible values for  $W_{ij}$ 
(1 for a win, .5 for a tie and 0 for a loss). 

For all these $W_{ji} = 1 - W_{ij}$ 

Since both $W_{ji}$ and $p_{ji}$ are both directly related to $W_{ij}$ and $p_{ij}$ and since $W_{ji} + W_{ij} = 1$ and $p_{ji} + p_{ij}= 1$ we know that the sum of the rankings will always remain equal

###2a. 

To show that the total rating of $R_{i} + R_{j} are equal after each update, we can start by looking at the update function itself of $R_{i} =  $R_{i} + $K($W_{ij} - $p_{ij}) and $R_{j} =  $R_{j} + $K($W_{ji} - $p_{ji}). 

Since the rating update formulas are linear, should the assumption of total ratings being equal hold true then 
$W_{ji} - $p_{ji} = -1 * ($W_{ij} - $p_{ij})
or 
$W_{ji} - $p_{ji} = - $W_{ij} + $p_{ij})

Since $p_{ji} is an expected win probability, we can also state that  $p_{ji} = 1 - $p_{ij}.   

There are 3 possible values for  $W_{ij} 
(1 for a win, .5 for a tie and 0 for a loss). 

For all these $W_{ji} = a $W_{ij} 

Substituting these values in we get 

 $W_{ji} - 1 - $p_{ij} =  - $W_{ij} + $p_{ij})
 
 $W_{ji} - 1 - $p_{ij} =  - $W_{ij} + $p_{ij})
 
which can reduce down to and showing that the sum of the two values will always remain equal. 
$W_{ij} - $p_{ij} = $W_{ij} - $p_{ij} 
>>>>>>> 0915f9763b3b964ef74458a0c6325640ba837f4d





###2b. 


```{r q2_base_elo_function, echo=TRUE, include = FALSE}

elo_x_win = function(r1, r2, s = 20, rating_dif = r2 - r1) {
  return( 
    1/(1+10^((rating_dif)/s)
       )
  )
}

elo_rating_update = function(points, r1, r2, s, K){
  
  return(r1 + K * (points - elo_x_win(r1, r2, s)))
}

```

Looking at the two prediction curves below it suggests expected win percentages follow a sigmoid/logistic function and the value of *s* impacts the grows rate or how quickly the expected win rate will change at different rating differences. In this instance the growth is less gradual for the elo than cdf and has a steeper slope around the inflection point. 


```{r q2b, echo=TRUE, include = TRUE, fig.keep="asis"}


rating_dif <- data.frame(rating_dif = c(-30:30))

rating_dif$x_wins <- elo_x_win(r1 = NA, r2 = NA, s= 10, rating_dif = c(-30:30))

rating_dif$cdf = 1 - pnorm(rating_dif$rating_dif, 0, 10)

colors <- c("Sepal Width" = "blue", "Petal Length" = "red", "Petal Width" = "orange")

rating_dif %>% ggplot(aes(x = rating_dif)) +
  geom_line(aes( y = x_wins, color = "elo"))  +
  geom_line(aes(y = cdf, color = "cdf")) +
  scale_color_manual(values = c("elo" = "black", "cdf" = "red")) +
  ggtitle("Comparing elo and cdf curves at s = 10") 

```


### 2c. 
below we can see a practical application calculating the log odds at s = 10 where r_j varies from 1-199. In the graph we can see that the log odds of expected wins is linearly related. Knowing this, one could specify a logistic regression with the formula being $points ~ team_i + team_j. 

```{r q2c, echo=TRUE, include = TRUE}

ratings <- data.frame(rating_i = 100 , rating_j = c(1:199))

ratings$x_wins <- elo_x_win(r1 = ratings$rating_i, r2 = ratings$rating_j, s= 10) 

ratings %>% mutate(logodds = log(x_wins/(1-x_wins))) %>% 
ggplot(aes(rating_j, logodds)) +
  geom_point() +
  ggtitle("Log odds of expected wins by oppontent rating when rating_i = 100")

```

### 2d. 

Below is the code for specifying and running the elo implementation 

```{r q2_elo_function, echo=TRUE, include = TRUE}

elo_x_win = function(r1, r2, s = 20) {
  1/(1+10^((r2 - r1)/s))
}

elo_rating_update = function(points, r1, r2, s, K){
  elo_x_win
  
 data[i, "pre_match_r"] + K * (data[i,"points"] - x_win)
}


elo_data_prep = function(data, date_column = "dates") {
  
  
  if(nrow(data %>% filter(result == "no result"))>0) {
    warning("data contains no result matches, those will be removed from the data set") 
  }
  
  ## Check if date_column for ordering exists in data frame 
  if(!date_column %in% names(data)) {
    stop(paste("Error: Provided date_column name '", date_column, "does not exist in the column names of provided data,
               please provide a valid column name to order by"))
  }
  
  data <- data %>% ungroup() %>% 
     filter(coalesce(result, "") != "no result") %>% 
  mutate(Won = outcome.winner == teams & coalesce(result, outcome.result, "") != "tie", 
         Loss = outcome.winner != teams & coalesce(result, outcome.result, "") != "tie",
         points = case_when(Won ~ 1, Loss ~ 0, TRUE ~ .5)
         ) %>% 
    arrange(!!sym(date_column))
  ##set initial rating for teams 

 
 #rank games by team in order of date to be able to easily pull the team's ranking prior to a game
 data <- data %>% group_by(teams, gender) %>% 
   mutate(team_game_rank = rank(!!sym(date_column))) %>% 
   ungroup() %>% 
   data.frame() ## set back to data.frame to deal with differences in dplyr vs base r filter/assignment syntax
  
 return(data)
}
  

##function to loop over dataframe and calculate elo 
elo_data = function(data, date_column = "dates", R0 = 100, s = 20, K = 20, prep_data = TRUE) {
  
  ## prep/check data if not assumption is that data is perfect. This is included to help speed up optimizations
 if(prep_data) {
  data = elo_data_prep(data, date_column)
 }
  
   data$pre_match_r = R0
  data$post_match_r = as.numeric(NA)
  data$x_win = as.numeric(NA)
 
  for(i in 1:nrow(data)) {
 
    # set row being used for reference
    row_use = data[i,]
    
    opponent_row = data %>% filter(matchid ==  row_use$matchid & teams !=  row_use$teams)

    if(is.na(opponent_row$x_win)) { 
    
      x_win =  elo_x_win(row_use$pre_match_r, opponent_row$pre_match_r, s = s)
    } else {
      x_win = 1 - opponent_row$x_win
    }
 
    data[i, c("x_win","post_match_r")] = c(x_win, data[i, "pre_match_r"] + K * (data[i,"points"] - x_win))

    ## check to see if a row exists/we're at the last game for a team. If not, then update their pregame rating for their next game

    if(!row_use$team_game_rank == max(data[data$teams == row_use$teams & 
                                           data$gender == row_use$gender, "team_game_rank"])) {
    data[data$team_game_rank == row_use$team_game_rank + 1 & 
           data$teams == row_use$teams & data$gender == row_use$gender, "pre_match_r"] = data[i, "post_match_r"] 
    }

  }


  return(data) 
}

  
```

``` {r q2d application, echo=TRUE, include = TRUE}

elo_data_use <- match_results %>% filter(gender == "male" & match_type == "ODI" & coalesce(result, "") != "no result") %>% 
  mutate(Won = outcome.winner == teams & coalesce(result, "") != "tie", 
         Loss = outcome.winner != teams & coalesce(result, "") != "tie"
         )

elo_results = elo_data(elo_data_use)



```


### 2e. 

In order to determine "optimal" choices for s and K we will need to decide on an evaluation metric. For this purpose I have chosen to use logloss comparing the wins for each team from a match against the expecting win% from the elo output. This isn't the perfect approach as it does not account for ties in matches, however since only 19 of 1829 matches ended in a tie it seemed like it is okay to treat them as losses for optimization sake.  

``` {r q2e_simulation, echo=TRUE, include = FALSE, cache=TRUE}

library(MLmetrics)
##check 

## function to run elo and return logloss 
optim_elo = function(K, s, data, ...) {
   elo_results <- elo_data(data, K = K, s = s, ...) 

  return(LogLoss(elo_results$x_win, as.numeric(elo_results$Won)))
}


optim_data_prep = elo_data_prep(elo_data_use)


results = data.frame(k=NA, s= NA, log_loss=as.numeric(NA))

## Note that  I changed around some of these starting values from where I initially started to get the optimizing function to converge faster when knitting the final document
start_vals = data.frame(k = 3, s= 10)

plan(multisession, workers = min(c(parallel::detectCores(), 9, na.rm = TRUE)))

reps = 16
for(i in 1:reps) {
  print(i)
  if(i == 1) {
    vals_use = start_vals
  } else {
    ## Get current min log loss 
    vals_use = results %>% filter(log_loss == min(log_loss, na.rm = TRUE))
  }
  
    print(vals_use)
    
  check_vals = expand.grid(k = c((vals_use$k - 1) : (vals_use$k + 1)), 
    s = c((vals_use$s - 1) : (vals_use$s + 1))
    )
  


  ## remove values that have been checked already
 check_vals_use = check_vals %>% anti_join(results, by = c("k", "s"))

 if(nrow(check_vals_use)==0) {
   print("Done! Reached optimal integer value, check results")
   break()
 }
 print(nrow(check_vals_use))
 

check_vals_use <- check_vals_use %>% 
  mutate(log_loss = furrr::future_map2_dbl(k, s, optim_elo, data = optim_data_prep, prep_data = FALSE))

  results = bind_rows(results, check_vals_use) %>% filter(!is.na(k))
  
}


results %>% filter(log_loss == min(log_loss, na.rm = TRUE)) %>% knitr::kable()


```

Based on the optimization results we find that a K value of 1 and s value of 13 minimizes the logloss between expected wins and actual outcomes for the dataset. This is a lower value of K than I expected and keeps the range of ratings to any any given point relatively small as well (between 89 and 111). What I would take away from this would be that cricket matches are inherently random and even the worse team will be victorious with some frequency.  For time sake I stuck with optimizing over integers only, but could see a K value below 1 being even more optimal and thus shrinking the rankings even further. Complete last rankings for teams can be seen in the table below. 

``` {r q2e_explore, echo=TRUE, include = TRUE}

opt = elo_data(elo_data_use, K=1, s=14)

opt %>% ungroup() %>%  group_by(teams) %>% 
  filter(dates == max(dates)) %>% 
  ungroup() %>% 
  select(Team = teams, Last_Date_Played = dates, Most_Recent_Ranking = post_match_r) %>% 
  arrange(desc(Most_Recent_Ranking)) %>% 
  knitr::kable()
```


## Question 3.  

There were a fair number of "gotcha's" in this data set as one might expect from any dataset. Overs being a character vs numeric led to changes needing to be made for sorting as the default sorting for characters would put "10.1" after "1.6" but before "2.1". There also were a handful of duplicate rows for counting wickets which needed to be accounted for. Notes on how these were handled are included in the code comments. 


After exploring a lot of different plot options (Including some animations of how the run distributions change by remaining opportunities), I ultimately decided upon using a filled time plot to display average runs per over by wickets and over remaining faceted by inning and included a plot of the difference between the two values as well. I limited the plot to only include inning/wicket/over remaining situations that had occurred at least 25 times in the data set to remove small sample outliers. 


``` {r q3_explore, echo=TRUE, include = TRUE}
## remove no result data. There also appears to be ~7800 rows with NA for matchid. I will remove those as well as there isn't a great way to know which rows belong to each match. 
inning_data_use = inning_results %>% filter(!is.na(matchid)) %>% 
  anti_join(match_results %>% 
           filter(coalesce(result, "") == "no result") %>% select(matchid) %>% unique(), by = "matchid") %>% 
  inner_join(match_results %>% 
           filter(gender == "male") %>% select(matchid) %>% unique(), by = "matchid") ## Include only male matches 


##There appear to be some duplicate rows caused by the wicket fielder column so next we'll remove that. There also seem to be one other instance of duplicates on a wicket but that did not seem to lead to double counting, as seen below

inning_data_use %>% filter(matchid==238214 & over=="2.1" & innings == 2)

## dedupe 
inning_data_use = inning_data_use %>% select(-wicket.fielders) %>% 
  distinct() 

## get pertinent information about overs and wickets remaining to summarise 
inning_data_use <- inning_data_use %>% 
  ## add over number by using the floor of the over and aggregate runs by over. 
  mutate(over_number = floor(as.numeric(over)), 
                             over_numeric = as.numeric(over), ##convert over to numeric for proper sorting of bowls 
         is_wicket = ifelse(is.na(wicket.kind), 0, 1)) %>%  ## add in a binary for if a wicket occured to be able to calculate remaining wickets. 
  arrange(matchid, innings, over_numeric) %>% 
  group_by(matchid,  innings) %>% 
  mutate(wickets_collected = cumsum(is_wicket) - is_wicket ) %>%
    ungroup() %>% 
  mutate(wickets_remaining = 10 - wickets_collected,
         overs_remaining = 50 - over_number) 

##get distinct instances of runs/over/wicket information with one row per over. 
runs_per_over <- inning_data_use %>% 
  group_by(matchid, over_number, innings) %>% 
  mutate(start_wickets_remaining = max(wickets_remaining),  ## get the wickets remaining at the start of the over. 
         total_runs = sum(runs.total), 
         bowls_per_over = n_distinct(over)- sum(!is.na(wides))) %>% ##get number of valid bowls per over to see if we need to account for "walkoff" overs 
  select(matchid, innings, over_number, overs_remaining, start_wickets_remaining, total_runs, bowls_per_over, team) %>% 
    ungroup() %>% 
  distinct()

match_runs <- runs_per_over %>% 
  group_by(matchid, innings) %>% 
  summarise(total_runs = sum(total_runs), 
            Overs_left = min(overs_remaining))

```

```{r q3_plot, echo=TRUE, include = TRUE, fig.width=7, fig.height=6, results = "asis"}

##get average runs per over by wickets and overs remaining 
runs_per_over_avg <- runs_per_over  %>% 
  group_by(start_wickets_remaining, overs_remaining, innings) %>% 
  summarise(  total_runs_complete = mean(ifelse(bowls_per_over == 6, total_runs, NA), na.rm = TRUE), ## get avg runs per over when it is a complete over to remove walkoffs/other incomplete overs. 
    total_runs = mean(total_runs), 
    total = n(), 
    avg_bowls_per_over = mean(bowls_per_over)
                                  ) 

run_avg_dif = runs_per_over_avg %>% 
  left_join(runs_per_over_avg, by = c("start_wickets_remaining", "overs_remaining")) %>% 
  filter(innings.x != innings.y & innings.x == 1) %>% 
  mutate(run_diff = total_runs.x - total_runs.y,
         run_diff_complete = total_runs_complete.x - total_runs_complete.y)

plot_base = runs_per_over_avg %>% 
  filter(total>=25) %>% 
  ggplot(aes(x=overs_remaining, y = start_wickets_remaining)) +
  geom_tile(aes(fill = total_runs)) +
  facet_wrap(~paste("Inning", innings), scales = "free") + 
  scale_fill_gradient2(midpoint = 5) + ## average runs/over is right around 5 across all matches 
  scale_y_reverse() +
  scale_x_reverse() +
  xlab("Remaining Overs") + 
  ylab("Wickets Remaining") + 
  labs(subtitle="Includes Results from incomplete Overs (fewer than 6 bowls). Min. 25 Game occurences", 
       fill = "Runs/Over") + 
  ggtitle("Runs per Over by Remaining Overs/Wickets and Inning Number") +
  theme_bw()

plt_dif = run_avg_dif %>% mutate(inning_label = "inning 1 - inning 2") %>% 
  filter(total.x >= 25 & total.y >= 25) %>% 
  ggplot(aes(x=overs_remaining, y = start_wickets_remaining)) +
  geom_tile(aes(fill = run_diff)) +
  facet_wrap(~inning_label) +
  scale_fill_gradient2(midpoint = 0, low = "red", high = "darkgreen") +
  scale_y_reverse() +
  scale_x_reverse() +
  xlab("Remaining Overs") + 
  ylab("Wickets Remaining") + 
  labs(subtitle="Includes Results from incomplete Overs (fewer than 6 bowls). Min. 25 Game occurences in each inning.", 
       fill = "Runs/Over Difference") + 
  ggtitle("Difference in Runs per Over between Inning 1 and 2") + 
  theme_bw()

library(cowplot)
plot_grid(plot_base , plt_dif, ncol = 1)

```


From this we can see that runs per over tend to increase as the number of overs remaining decreases and that runs tend to decrease as the number of wickets remain. We also see that run scoring per over tends to be more widely spread in inning 1. Particularly as the number of overs remaining decreases. On a whole one would think that the increase in scoring as overs remaining decrease may be due to fatigue from the bowlers and increased familiarity from batters as to what their bowls look like (similar to what we see in baseball with times through the order penalties). The difference in run scoring late in matches in inning two is potentially due to the batting team in inning 2 having a target number of runs they need to score and as such are less likely to have high run innings at the end of the match because once they surpass the opposing team's run total then the game would conclude. It is harder to come up with concrete theories for why wickets remaining has an effect on run scoring that isn't related to sample bias in that teams with better bowlers/fielders are more likely to find themselves in situations with fewer wickets remaining and are thus more likely to continue preventing runs/getting wickets. Similar sample bias may exist on the overs remaining as well with better batting/worse bowling teams being more likely to find themselves in situations where they have reached a high number of overs without giving up many wickets, thus would be expected to continue to perform above the average. 

In the next section it will be difficult to account for all of these confounding variables, but we do see some clear patterns in terms of how game state may impact run scoring and that combined with some basic quality of competition adjustments should allow us to develop a reasonable model for average run scoring. 

## Question 4. 

When looking at the distribution of runs per over the shape appears fit within a poisson distribution with that and the potential interactive and non-linear effects of overs and wickets remaining, I settled on building a gam model. An added benefit of this modelling framework is that it allows for specification of random effects as well so we can add in rough controls for the batting and bowling team identies along with any other potential confounding variables. Given that the visualization in question 3 was limited to male matches, I opted to limit this model to the same group of matches. 

I experimented with including specific bowler/batsmen identities in the model as random effects, however due to time and compute limitations I opted to exclude them. In a more developed model with adequate resourcing I would anticipate that their inclusion would be beneficial and could provide some insight into individual player skill as a beneficial side effect. 

To validate the model and select an ideal formula and parameters, I did a split half validation, splitting on games played in even/odd days of the year. I chose to ensure that each match was limited to only one half of the validation set as I did not want any model leakage from that, though I did consider splitting on even/odd over numbers. 

One model specification I went back and forth on was whether to specify the interactive smooth of overs and wickets remaining as a tensor spline so I did a test of the two formulas with a little parameter tuning on the k values. The differences ended up being negligible in the smooths so I opted for the s() formula. In the end the model provided a 6% improvement in RMSE over the naive model of using average runs per over. This is a small gain and I think speaks to the difficulty in accurately predicting runs on any given over. I would anticipate over larger aggregate samples that a model like this may perform better than on any given over. 


```{r q4_dev, echo=TRUE, include = TRUE, cache=TRUE}

library(mgcv)

## pull in player data from first bowl of the over 
start_inning_data_join <- inning_data_use %>% 
               filter(substring(over, nchar(over), nchar(over)) == 1) %>% 
    group_by(matchid, innings, over_number) %>% 
    mutate(row_rank = rank(over, ties.method = "first")) %>% 
   ungroup() %>% 
   filter(row_rank == 1) %>% 
              select(bowler, over_number, batsman, non_striker, matchid, innings) 

## prepare model data for use 
dat_use = runs_per_over %>% ungroup() %>% 
  left_join(match_results %>% select(matchid, dates, match_type, bowl_team = teams), by = "matchid") %>% 
  filter(team != bowl_team) %>% 
  left_join( start_inning_data_join, ## get first bowler,batsman and non_striker of over
            by = c("matchid", "innings", "over_number")) %>% 
  mutate(Season = lubridate::year(dates),
         Is_Inning2 = as.numeric(innings == 2),
         across(any_of(c("bowl_team", "team", "bowler", "batsman", "non_striker","Season")), as.factor)

         ) 

 ##Check naive model of average runs per over for baseline RMSE 
  
RMSE(dat_use$total_runs, 5.04)
 
form = as.formula("total_runs ~ 1 + Is_Inning2 +  s(overs_remaining, start_wickets_remaining, k=100) + s(team, bs='re', k=50) + s(bowl_team, bs='re', k = 50) + s(Season, bs='re', k = 50)")

form2 = as.formula("total_runs ~ 1 + Is_Inning2 +  te(overs_remaining, start_wickets_remaining, k=c(7,7)) + s(team, bs='re', k=50) + s(bowl_team, bs='re', k = 50) + s(Season, bs='re', k = 50)")



## create function to be able to map over in dataframe for split half validation 
map_gam = function(data, form, family="poisson", ...) {
 
  gam(form, family=family,
          data = data, ...)
  
}


val_data <- dat_use %>% 
  mutate(split_half = over_number%%2) %>% ## use even/odd day of year for split half validation 
  group_by(split_half) %>% 
  tidyr::nest() %>% 
    crossing(tibble(form = list(form, form2)))

plan(multisession, workers = min(c(nrow(val_data), parallel::detectCores(), na.rm = TRUE)))

val_data <- val_data %>% 
  mutate(models = furrr::future_map2(data, form, map_gam))

val_data <- val_data %>%   
   mutate(form_text = map_chr(form, ~as.character(.x) %>% .[3]))

val_data_check =  val_data %>% left_join(val_data %>% 
                           mutate(split_match = abs(split_half - 1)) %>% select(split_match, models, form_text),
                         by = c("split_half" = "split_match", "form_text" = "form_text")) 


val_data_check <- val_data_check  %>% mutate(pred =  furrr::future_map2(models.y, data, predict, type = "response" , 
                                                                        newdata.guaranteed = TRUE))

val_data_check %>% select(split_half, data, pred, form_text) %>% 
  tidyr::unnest(cols = c(data, pred)) %>% 
  group_by(split_half, form_text) %>% 
  summarise(RMSE = RMSE(total_runs, pred)) %>% arrange(RMSE) %>% 
  knitr::kable() 



fit = gam(form, family="poisson",
          data = dat_use)

summary(fit)

```


Using the intercept and innings values for the model we can derive the "average team's" expected run per over. For inning 1 the average expected runs per over is `r round(exp(fit$coefficients[1]),2)` runs and in inning 2 the average expected runs per over is `r round(exp(fit$coefficients[1] + fit$coefficients[2]),2)` runs. Averaging those two together (Acknowledging that there are likely more actual overs in inning 1, but for the purpose of this I believe we should ignore that assumption), we get an estimate of `r round((exp(fit$coefficients[1]) + (exp(fit$coefficients[1] + fit$coefficients[2])))/2,2) ` runs per over. 

## Question 5. 

```{r q5,  echo=TRUE, include = TRUE}

rank_results <- opt %>% 
  filter(lubridate::year(dates) == 2020) %>% 
  mutate(surprise_rank = rank(desc(points - x_win))) %>% 
  filter(surprise_rank <=5) %>% 
  arrange(surprise_rank) %>% 
  left_join(opt %>% select( opponent = teams, matchid, opponent_rank = pre_match_r), by = "matchid") %>% 
  filter(opponent != teams) %>% 
  select(dates, teams, team_rank = pre_match_r, opponent, opponent_rank,  x_win, points) %>% 
  mutate(x_win =  scales::percent(x_win, .1), team_rank = round(team_rank, 2), 
         opponent_rank =  round(opponent_rank, 2)) 

```

For defining the most surprising win for men's ODI in 2020 I opted to take a simple approach and utilize the results from the ELO model that was optimized in question 2. Using that model we are able to extract the pre-match win expectation for each match in 2020 based on the team's elo rating and determine which team was victorious while having the lowest expected win probability. With this criteria we find that `r rank_results$teams[1]` defeating `r rank_results$opponent[1]` on `r rank_results$dates[1]` was the most surprising win as they only had an expected win% of `r rank_results$x_win[1]`. A full table of results is below. 

```{r q5_kable,  echo=TRUE, include = TRUE}

rank_results %>% 
  knitr::kable()


```

